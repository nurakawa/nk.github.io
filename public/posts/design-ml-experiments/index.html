<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content="A detailed guideline for designing machine learning experiments that produce reliable, reproducible results."/>

<title>
    
    Machine Learning Experiments Done Right | Nura Kawa
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/design-ml-experiments/"/>

<meta property="og:url" content="http://localhost:1313/posts/design-ml-experiments/">
  <meta property="og:site_name" content="Nura Kawa">
  <meta property="og:title" content="Machine Learning Experiments Done Right">
  <meta property="og:description" content="A detailed guideline for designing machine learning experiments that produce reliable, reproducible results.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-02T00:00:00+00:00">
    <meta property="article:tag" content="Database">
    <meta property="article:tag" content="Java">













<link rel="stylesheet" href="/assets/combined.min.f1ad03b67a5380b14dc1bf13677331e49fdce064e1619b80e8e0b2e90a1685e0.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">
        <a href="http://localhost:1313/">Nura Kawa</a>
    </h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/projects" >
                /projects
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        





<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a href="/posts/">Posts</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/posts/design-ml-experiments/">Machine Learning Experiments Done Right</a>
</div>



<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Machine Learning Experiments Done Right</h1>
    
    <p class="single-summary">A detailed guideline for designing machine learning experiments that produce reliable, reproducible results.</p>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-12-02T00:00:00&#43;00:00">December 2, 2024</time>
      

      
      &nbsp; · &nbsp;
      7 min read
      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <p><em>Originally posted on <a href="https://medium.com/p/6ed04f5e959b">Medium</a>.</em></p>
<p>Machine learning (ML) practitioners run experiments to compare the effectiveness of methods for both specific applications and for general types of problems. The validity of experimental results hinges on how practitioners design, run, and analyze their experiments. Unfortunately, many ML papers lack valid results. Recent studies [5] [6] reveal a lack of reproducibility in published experiments, attributing this to practices such as:</p>
<ul>
<li>Data contamination: engineering training datasets to include data that is semantically similar to, or directly from, the test dataset</li>
<li>Cherrypicking: selectively choosing an experimental setup or results that favorably present a method</li>
<li>Misreporting: including “the improper use of statistics to analyze results, such as claiming significance without proper statistical testing or using the wrong statistic test [6]</li>
</ul>
<p>Such practices are not necessarily done intentionally — practitioners may face pressure to produce quick results or lack adequate resources. However, consistently using poor experimental practices inevitably leads to costly outcomes. So, how should we conduct Machine Learning experiments that achieve reproducible and reliable results? In this post, we present a guideline for designing and executing rigorous Machine Learning experiments.</p>
<h2 id="experiments-factors-and-response-functions">Experiments: factors and response functions</h2>
<p>An experiment involves a system with an input, a process, and an output, visualized in the diagram below. Consider a garden as a simple example: bulbs are the input, germination is the process, and flowers are the output. In an ML system, data is input into a learning function, which outputs predictions.</p>
<p>A practitioner aims to maximize some response function of the output — in our garden example, this could be the number of blooming flowers, while in an ML system, this is usually model accuracy. This response function depends on both controllable and uncontrollable factors. A gardener can control soil quality and daily watering but cannot control the weather. An ML practitioner can control most parameters in a ML system, such as the training procedure, parameters and pre-processing steps, while randomness comes from data selection.</p>
<p>










<figure class="">

    <div>
        <img loading="lazy" alt="Diagram of an experiment, based on [2]" src="../../ml-experiments/experiment-setup.png">
    </div>

    
</figure></p>
<p>The goal of an experiment is to find the best configuration of controllable factors that maximizes the response function while minimizing the impact of uncontrollable factors. A well-designed experiment needs two key elements: a systematic way to test different combinations of controllable factors, and a method to account for randomness from uncontrollable factors.</p>
<p>Building on these principles, a clear and organized framework is crucial for effectively designing and conducting experiments. Below, we present a checklist that guides a practitioner through the planning and execution of an ML experiment.</p>
<h2 id="a-machine-learning-experiment-checklist">A Machine Learning Experiment Checklist</h2>
<p><em>To plan and perform a rigorous ML experiment:</em></p>
<ol>
<li>State the objective of your experiment</li>
<li>Select the response function, or what you want to measure</li>
<li>Decide what factors vary, and what remains the same</li>
<li>Describe one run of the experiment, which should define:<br>
(a) a single configuration of the experiment<br>
(b) the datasets used</li>
<li>Choose an experimental design, which should define:
(a) how we explore the factor space and<br>
(b) how we repeat our measurements (cross validation)</li>
<li>Perform the experiment</li>
<li>Analyze the data</li>
<li>Draw conclusions and recommendations</li>
</ol>
<h3 id="1-state-the-objective-of-the-experiment">1. State the objective of the experiment</h3>
<p>The objective should state clearly why is the experiment to be performed. It is also important to specify a meaningful effect size. For example, if the goal of an experiment is “to determine the if using a data augmentation technique improves my model’s accuracy”, then we must add, “a significant improvement is greater than or equal to 5%.”</p>
<h3 id="2-select-the-response-function-or-what-you-want-to-measure">2. Select the response function, or what you want to measure</h3>
<p>The response function of a Machine Learning experiment is typically an accuracy metric relative to the task of the learning function, such as classification accuracy, mean average precision, or mean squared error. It could also be a measure of interpretability, robustness or complexity — so long as the metric is be well-defined.</p>
<h3 id="3-decide-what-factors-vary-and-what-remains-the-same">3. Decide what factors vary, and what remains the same</h3>
<p>A machine learning system has several controllable factors, such as model design, data pre-processing, training strategy, and feature selection. In this step, we decide what factors remain static, and what can vary across runs. For example, if the objective is “to determine the if using a data augmentation technique improves my model’s accuracy”, we could choose to vary the data augmentation strategies and their parameters, but keep the model the same across all runs.</p>
<h3 id="4-describe-one-run-of-the-experiment">4. Describe one run of the experiment</h3>
<p>A run is a single instance of the experiment, where a process is applied to a single configuration of factors. In our example experiment with the aim “to determine the if using a data augmentation technique improves my model’s accuracy”, a single run would be: “to train a model on a training dataset using one data augmentation technique and measure its accuracy on a held-out test set.”</p>
<p>In this step, we also select the data for our experiment. When choosing datasets, we must consider whether our experiment a domain-specific application or for generic use. A domain-specific experiment typically requires a single dataset that is representative of the domain, while experiments that aim to show a generic result should evaluate methods across multiple datasets with diverse data types [1].</p>
<p>In both cases, we must define specifically the training, validation and testing datasets. If we are splitting one dataset, we should record the data splits. This is an essential step in avoiding accidental contamination!</p>
<h3 id="5-choose-an-experimental-design">5. Choose an experimental design</h3>
<p>The experimental design is is the collection of runs that we will perform. An experiment design describes:</p>
<p>What factors and levels (categories or values of a factor) will be studied
A randomization scheme (cross validation)
If we are running an experiment to test the impact of training dataset size on the resulting model’s robustness, which range of sizes will we test, and how granular should we get? When varying multiple factors, does it make sense to test all possible combinations of all factor/level configurations? If we plan to perform statistical tests, it could be helpful to follow a specific experiment design, such as a factorial design or randomized block design (see [3] for more information).</p>
<p>Cross validation is essential for ML experiments, as this reduces the variance of your results which come from the choice of dataset split. To determine the number of cross-validation samples needed, we return to our objective statement in Step 1. If we plan to perform a statistical analysis, we need to ensure that we generate enough data for our specific statistical test.</p>
<p>A final part of this step is to think about resource constraints. How much time and compute does one run take? Do we have enough resources to run this experiment as we designed it? Perhaps the design must be altered to meet resource constraints.</p>
<h3 id="6-perform-the-experiment">6. Perform the experiment</h3>
<p>To ensure that the experiment runs smoothly, It is important to have a rigorous system in place to organize data, track experiment runs, and analyze resource allocation. Several open-source tools are available for this purpose (see awesome-ml-experiment-management).</p>
<h3 id="7-analyze-the-data">7. Analyze the data</h3>
<p>Depending on the objective and the domain of the experiment, it could suffice to look at cross-validation averages (and error bars!) of the results. However, the best way to validate results is through statistical hypothesis testing, which rigorously shows that the probability of obtaining your results given the data is not due to chance. Statistical testing is necessary if the objective of the experiment is to show a cause-and-effect relationship.</p>
<h3 id="8-draw-conclusions">8. Draw conclusions</h3>
<p>Depending on the analysis in the previous step, we can now state the conclusions we draw from our experiment. Can we make any claims from our results, or do we need to see more data? Solid conclusions are backed by the resulting data and are reproducible. Any practitioner who is unfamiliar with the experiment should be able to run the experiment from start to finish, obtain the same results, and draw from the results the same conclusions.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>A Machine Learning experiment has two key factors: a systematic design for testing different combinations of factors, and a cross-validation scheme to control for randomness. Following the ML experiment checklist of this post throughout the planning and execution of an experiment can help a practitioner, or a team of practitioners, ensure that the resulting experiments are reliable and reproducible.</p>
<p>Thank you for reading! If you found this post useful, please consider following me on Medium, or checking out my website.</p>
<h2 id="references">References</h2>
<p>[1] Joris Guerin “A Quick Guide to Design Rigorous Machine Learning Experiments.” Towards Data Science. Available Online.</p>
<p>[2] Design &amp; Analysis of Machine Learning Experiments — Machine Learning — Spring 2016 — Professor Kogan. YouTube video.</p>
<p>[3] Lawson, John. Design and analysis of experiments with R. Available Online.</p>
<p>[4] Questionable Practices in Machine Learning. ArXiv preprint.</p>
<p>[5] Improving Reproducibility in Machine Learning Research. Journal of Machine Learning Research, 2022. Available Online.</p>
<p>[6] A Step Toward Quantifying Independently Reproducible Machine Learning Research. ArXiv preprint.</p>

    
  </div>

  


  

  
  

<div class="single-pagination">
    <hr />

    <div class="flex">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">←</div>
                <div class="single-pagination-text">
                    <a href="/posts/linear-regression/">
                        Linear Regression
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      

    
    <p>Powered by
        <a href="https://gohugo.io/">Hugo</a>
        theme based on 
        <a href="https://github.com/tomfran/typo">tomfran/typo</a>
    </p>
    
    
    



<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ]
    });
  });
</script>


    </footer>
    
  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>